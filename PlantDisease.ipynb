{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a324840d",
   "metadata": {
    "id": "a324840d",
    "papermill": {
     "duration": 12.98067,
     "end_time": "2023-12-02T06:53:20.245439",
     "exception": false,
     "start_time": "2023-12-02T06:53:07.264769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 21:37:06.622665: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-10 21:37:06.790956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-10 21:37:06.790987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-10 21:37:06.816098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-10 21:37:06.873033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-10 21:37:07.580897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.set_random_seed(0)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import itertools\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331b47b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "331b47b0",
    "outputId": "c5b91681-1370-4817-8753-97a6ccf808c8",
    "papermill": {
     "duration": 0.016242,
     "end_time": "2023-12-02T06:53:20.264429",
     "exception": false,
     "start_time": "2023-12-02T06:53:20.248187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf23fe1-8bb0-4fd3-ab02-cfed5dafdd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading new-plant-diseases-dataset.zip to /mnt/d/Plant Disease Detection\n",
      "100%|█████████████████████████████████████▉| 2.70G/2.70G [02:47<00:00, 18.8MB/s]\n",
      "100%|██████████████████████████████████████| 2.70G/2.70G [02:47<00:00, 17.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d vipoooool/new-plant-diseases-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25340a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d25340a9",
    "outputId": "7f035fc6-ddf1-49a5-f403-b089c6093638",
    "papermill": {
     "duration": 32.055372,
     "end_time": "2023-12-02T06:53:52.322302",
     "exception": false,
     "start_time": "2023-12-02T06:53:20.266930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70295 files belonging to 38 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 21:38:12.531557: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.638376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.638415: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.640490: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.640527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.640545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.733515: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.733562: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.733568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-10 21:38:12.733593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-10 21:38:12.733607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-02-10 21:38:13.379771: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17572 files belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_gen = image_dataset_from_directory(directory=\"Data/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\",\n",
    "                                         image_size=(256, 256))\n",
    "test_gen = image_dataset_from_directory(directory=\"Data/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid\",\n",
    "                                        image_size=(256, 256))\n",
    "\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_gen = train_gen.map(lambda image,label:(rescale(image),label))\n",
    "test_gen  = test_gen.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce481eb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce481eb2",
    "outputId": "673fb34d-c2b4-4d04-ff97-9567d06235b7",
    "papermill": {
     "duration": 0.167388,
     "end_time": "2023-12-02T06:53:52.493442",
     "exception": false,
     "start_time": "2023-12-02T06:53:52.326054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 85, 85, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 85, 85, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 42, 42, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 112896)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1568)              177022496 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 38)                59622     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177101510 (675.59 MB)\n",
      "Trainable params: 177101510 (675.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(256, 256, 3)))\n",
    "model.add(keras.layers.MaxPooling2D(3, 3))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
    "model.add(keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(1568, activation=\"relu\"))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(38, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde90277",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dde90277",
    "outputId": "7c9c27c9-e9ad-4d56-888c-f324f2bc19ad",
    "papermill": {
     "duration": 1109.40411,
     "end_time": "2023-12-02T07:12:21.902268",
     "exception": false,
     "start_time": "2023-12-02T06:53:52.498158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 21:38:31.813223: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-10 21:38:32.082575: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-10 21:38:33.091646: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa7a84cd9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-10 21:38:33.091673: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-02-10 21:38:33.100378: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707581313.177837     641 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2197/2197 [==============================] - 176s 79ms/step - loss: 1.3954 - accuracy: 0.6006 - val_loss: 0.6235 - val_accuracy: 0.8206\n",
      "Epoch 2/10\n",
      "2197/2197 [==============================] - 167s 76ms/step - loss: 0.6548 - accuracy: 0.8008 - val_loss: 0.4058 - val_accuracy: 0.8770\n",
      "Epoch 3/10\n",
      "2197/2197 [==============================] - 173s 79ms/step - loss: 0.4599 - accuracy: 0.8565 - val_loss: 0.3007 - val_accuracy: 0.9065\n",
      "Epoch 4/10\n",
      "2197/2197 [==============================] - 777s 354ms/step - loss: 0.3477 - accuracy: 0.8912 - val_loss: 0.2452 - val_accuracy: 0.9228\n",
      "Epoch 5/10\n",
      "2197/2197 [==============================] - 167s 76ms/step - loss: 0.2757 - accuracy: 0.9125 - val_loss: 0.2176 - val_accuracy: 0.9301\n",
      "Epoch 6/10\n",
      "2197/2197 [==============================] - 166s 76ms/step - loss: 0.2199 - accuracy: 0.9301 - val_loss: 0.1991 - val_accuracy: 0.9347\n",
      "Epoch 7/10\n",
      "2197/2197 [==============================] - 167s 76ms/step - loss: 0.1791 - accuracy: 0.9421 - val_loss: 0.1797 - val_accuracy: 0.9426\n",
      "Epoch 8/10\n",
      "2197/2197 [==============================] - 166s 76ms/step - loss: 0.1479 - accuracy: 0.9522 - val_loss: 0.1739 - val_accuracy: 0.9425\n",
      "Epoch 9/10\n",
      "2197/2197 [==============================] - 167s 76ms/step - loss: 0.1250 - accuracy: 0.9588 - val_loss: 0.1587 - val_accuracy: 0.9483\n",
      "Epoch 10/10\n",
      "2197/2197 [==============================] - 166s 75ms/step - loss: 0.1070 - accuracy: 0.9661 - val_loss: 0.1621 - val_accuracy: 0.9461\n"
     ]
    }
   ],
   "source": [
    "ep = 10\n",
    "history = model.fit_generator(train_gen,\n",
    "          validation_data=test_gen,\n",
    "          epochs = ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZzBdgds1P4wh",
   "metadata": {
    "id": "ZzBdgds1P4wh"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686e55d9-4033-4c83-877d-218ab28ef32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfb433-d516-4518-acb6-592985a38bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 78313,
     "sourceId": 182633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1162.295286,
   "end_time": "2023-12-02T07:12:26.166881",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-02T06:53:03.871595",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
